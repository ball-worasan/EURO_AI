{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe3543b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Loaded meta keys: dict_keys(['csv_path', 'seq_len', 'horizon', 'train_ratio', 'val_ratio', 'feature_cols', 'columns_required', 'targets_boosting', 'targets_dl_true', 'note'])\n",
      "Feature cols: 24\n",
      "Targets boosting: ['gap_next', 'range_next', 'body_next']\n",
      "Targets DL true: ['gap_next', 'range_next', 'body_next', 'upper_wick_next', 'lower_wick_next']\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Call 1: ตั้งค่าเริ่มต้น + import + โหลด meta\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---- paths ----\n",
    "DATA_DIR = Path(\n",
    "    \"/Users/thanaporn/Desktop/EURO_H1_AI/prepared_datasets/boosting_dl_residual\"\n",
    ")\n",
    "NPZ_PATH = DATA_DIR / \"eurusd_struct_sequences.npz\"\n",
    "META_PATH = DATA_DIR / \"eurusd_struct_meta.json\"\n",
    "\n",
    "with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "print(\"Loaded meta keys:\", meta.keys())\n",
    "print(\"Feature cols:\", len(meta[\"feature_cols\"]))\n",
    "print(\"Targets boosting:\", meta[\"targets_boosting\"])\n",
    "print(\"Targets DL true:\", meta[\"targets_dl_true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16eb45b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting train: (2912, 24) (2912, 3)\n",
      "DL train: (2895, 24, 24) (2895, 5)\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Call 2: โหลด NPZ dataset\n",
    "# ============================\n",
    "\n",
    "npz = np.load(NPZ_PATH, allow_pickle=True)\n",
    "\n",
    "# ---- Boosting tabular ----\n",
    "Xb_train = npz[\"Xb_train\"]\n",
    "Xb_val = npz[\"Xb_val\"]\n",
    "Xb_test = npz[\"Xb_test\"]\n",
    "\n",
    "yb_train = npz[\"yb_train\"]  # [N,3] = gap_next, range_next, body_next\n",
    "yb_val = npz[\"yb_val\"]\n",
    "yb_test = npz[\"yb_test\"]\n",
    "\n",
    "idxb_train = npz[\"idxb_train\"]\n",
    "idxb_val = npz[\"idxb_val\"]\n",
    "idxb_test = npz[\"idxb_test\"]\n",
    "\n",
    "# ---- DL sequences ----\n",
    "Xs_train = npz[\"Xs_train\"]  # [N, seq_len, F]\n",
    "Xs_val = npz[\"Xs_val\"]\n",
    "Xs_test = npz[\"Xs_test\"]\n",
    "\n",
    "yd_train_true = npz[\"yd_train_true\"]  # [N,5] = gap,range,body,uw,lw (true)\n",
    "yd_val_true = npz[\"yd_val_true\"]\n",
    "yd_test_true = npz[\"yd_test_true\"]\n",
    "\n",
    "idxs_train = npz[\"idxs_train\"]\n",
    "idxs_val = npz[\"idxs_val\"]\n",
    "idxs_test = npz[\"idxs_test\"]\n",
    "\n",
    "print(\"Boosting train:\", Xb_train.shape, yb_train.shape)\n",
    "print(\"DL train:\", Xs_train.shape, yd_train_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecf527ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4618\n",
      "[LightGBM] [Info] Number of data points in the train set: 2912, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -0.000018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanaporn/miniconda3/envs/forex-gpu/lib/python3.11/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained boosting target: gap_next, best_iter=8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4618\n",
      "[LightGBM] [Info] Number of data points in the train set: 2912, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.009904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanaporn/miniconda3/envs/forex-gpu/lib/python3.11/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained boosting target: range_next, best_iter=137\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4618\n",
      "[LightGBM] [Info] Number of data points in the train set: 2912, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -0.000058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanaporn/miniconda3/envs/forex-gpu/lib/python3.11/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained boosting target: body_next, best_iter=11\n",
      "[Boost train] MAE: {'gap_next': np.float64(0.00032645472697125614), 'range_next': np.float64(0.0021351013930006545), 'body_next': np.float64(0.004817963623254481)}\n",
      "[Boost train] RMSE: {'gap_next': np.float64(0.0012012072929406489), 'range_next': np.float64(0.0030708938555879203), 'body_next': np.float64(0.006557385153565503)}\n",
      "[Boost val] MAE: {'gap_next': np.float64(0.00023072437597563967), 'range_next': np.float64(0.0024945631913204282), 'body_next': np.float64(0.004142512703122564)}\n",
      "[Boost val] RMSE: {'gap_next': np.float64(0.0006999679107057043), 'range_next': np.float64(0.003344482495706121), 'body_next': np.float64(0.005418909247105769)}\n",
      "[Boost test] MAE: {'gap_next': np.float64(0.000235636980620905), 'range_next': np.float64(0.0025889435095274175), 'body_next': np.float64(0.0035548009195827666)}\n",
      "[Boost test] RMSE: {'gap_next': np.float64(0.0007902346938645659), 'range_next': np.float64(0.0034403079437801683), 'body_next': np.float64(0.004828084622803364)}\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Call 3: Train Boosting (LightGBM 3 โมเดล) + ทำ pred ทั้งชุด\n",
    "# ============================\n",
    "\n",
    "target_names = meta[\"targets_boosting\"]  # [\"gap_next\",\"range_next\",\"body_next\"]\n",
    "\n",
    "params = dict(\n",
    "    objective=\"regression\",\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.02,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "boost_models = []\n",
    "\n",
    "\n",
    "def train_one_target(i: int):\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        Xb_train,\n",
    "        yb_train[:, i],\n",
    "        eval_set=[(Xb_val, yb_val[:, i])],\n",
    "        eval_metric=\"l2\",\n",
    "        callbacks=[lgb.early_stopping(200, verbose=False)],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "for i, tname in enumerate(target_names):\n",
    "    m = train_one_target(i)\n",
    "    boost_models.append(m)\n",
    "    print(f\"Trained boosting target: {tname}, best_iter={m.best_iteration_}\")\n",
    "\n",
    "\n",
    "# ---- predictions for all splits ----\n",
    "def predict_boost(X_np):\n",
    "    X_df = pd.DataFrame(X_np, columns=meta[\"feature_cols\"])\n",
    "    preds = []\n",
    "    for m in boost_models:\n",
    "        preds.append(m.predict(X_df, num_iteration=m.best_iteration_))\n",
    "    return np.stack(preds, axis=1)  # [N,3]\n",
    "\n",
    "\n",
    "yb_pred_train = predict_boost(Xb_train)\n",
    "yb_pred_val = predict_boost(Xb_val)\n",
    "yb_pred_test = predict_boost(Xb_test)\n",
    "\n",
    "\n",
    "# ---- evaluation ----\n",
    "def eval_boost(name, y_true, y_pred):\n",
    "    maes = mean_absolute_error(y_true, y_pred, multioutput=\"raw_values\")\n",
    "    rmses = np.sqrt(mean_squared_error(y_true, y_pred, multioutput=\"raw_values\"))\n",
    "    print(f\"[Boost {name}] MAE:\", dict(zip(target_names, maes)))\n",
    "    print(f\"[Boost {name}] RMSE:\", dict(zip(target_names, rmses)))\n",
    "\n",
    "\n",
    "eval_boost(\"train\", yb_train, yb_pred_train)\n",
    "eval_boost(\"val\", yb_val, yb_pred_val)\n",
    "eval_boost(\"test\", yb_test, yb_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "35fa6128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL residual y shapes: (2895, 5) (620, 5) (621, 5)\n",
      "Scaled DL X: (2895, 24, 24)\n",
      "Alignment check (train): (2895, 5) (2895, 3)\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Call 4: Align boosting preds -> make residual -> scale DL inputs\n",
    "# ============================\n",
    "\n",
    "# ---- 1) รวม tabular ทั้งหมด แล้ว predict boosting ทั้งชุด ----\n",
    "Xb_all = np.concatenate([Xb_train, Xb_val, Xb_test], axis=0)\n",
    "yb_pred_all = predict_boost(Xb_all)  # shape [N_tab, 3]\n",
    "\n",
    "# ---- 2) รวม DL true targets ทั้งหมด ----\n",
    "yd_true_all = np.concatenate([yd_train_true, yd_val_true, yd_test_true], axis=0)\n",
    "N_dl = len(yd_true_all)\n",
    "\n",
    "# ---- 3) คำนวณ offset ของ DL ต่อ tabular ----\n",
    "# DL เริ่มใช้ target ที่แถว i = seq_len-1 ของ tabular\n",
    "offset = meta[\"seq_len\"] - 1\n",
    "\n",
    "# ---- 4) slice boosting preds ให้ตรงกับ DL timeline ----\n",
    "yb_pred_dl_all = yb_pred_all[offset : offset + N_dl]\n",
    "assert (\n",
    "    len(yb_pred_dl_all) == N_dl\n",
    "), f\"Aligned pred length mismatch: {len(yb_pred_dl_all)} vs {N_dl}\"\n",
    "\n",
    "# ---- 5) แยกกลับเป็น train/val/test ตามความยาว DL จริง ----\n",
    "n_tr = len(yd_train_true)\n",
    "n_va = len(yd_val_true)\n",
    "n_te = len(yd_test_true)\n",
    "\n",
    "yb_pred_train_aligned = yb_pred_dl_all[:n_tr]\n",
    "yb_pred_val_aligned = yb_pred_dl_all[n_tr : n_tr + n_va]\n",
    "yb_pred_test_aligned = yb_pred_dl_all[n_tr + n_va :]\n",
    "\n",
    "assert yb_pred_train_aligned.shape[0] == n_tr\n",
    "assert yb_pred_val_aligned.shape[0] == n_va\n",
    "assert yb_pred_test_aligned.shape[0] == n_te\n",
    "\n",
    "\n",
    "# ---- 6) ทำ residual targets สำหรับ DL ----\n",
    "def make_residual(yd_true_split, yb_pred_split):\n",
    "    yd = yd_true_split.copy()\n",
    "    yd[:, 0:3] = yd[:, 0:3] - yb_pred_split  # res_gap,res_range,res_body\n",
    "    return yd\n",
    "\n",
    "\n",
    "yd_train = make_residual(yd_train_true, yb_pred_train_aligned)\n",
    "yd_val = make_residual(yd_val_true, yb_pred_val_aligned)\n",
    "yd_test = make_residual(yd_test_true, yb_pred_test_aligned)\n",
    "\n",
    "print(\"DL residual y shapes:\", yd_train.shape, yd_val.shape, yd_test.shape)\n",
    "\n",
    "# ---- 7) standardize DL inputs (fit scaler on train only) ----\n",
    "B, T, F = Xs_train.shape\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xs_train.reshape(-1, F))\n",
    "\n",
    "\n",
    "def scale_seq(Xs):\n",
    "    Xflat = Xs.reshape(-1, F)\n",
    "    Xflat = scaler.transform(Xflat)\n",
    "    return Xflat.reshape(Xs.shape)\n",
    "\n",
    "\n",
    "Xs_train_s = scale_seq(Xs_train)\n",
    "Xs_val_s = scale_seq(Xs_val)\n",
    "Xs_test_s = scale_seq(Xs_test)\n",
    "\n",
    "print(\"Scaled DL X:\", Xs_train_s.shape)\n",
    "print(\"Alignment check (train):\", yd_train.shape, yb_pred_train_aligned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a9058871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCN(\n",
      "  (tcn): Sequential(\n",
      "    (0): TCNBlock(\n",
      "      (net): Sequential(\n",
      "        (0): Conv1d(24, 64, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (4): ReLU()\n",
      "        (5): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (down): Conv1d(24, 64, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): TCNBlock(\n",
      "      (net): Sequential(\n",
      "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (4): ReLU()\n",
      "        (5): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (down): Identity()\n",
      "    )\n",
      "    (2): TCNBlock(\n",
      "      (net): Sequential(\n",
      "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (4): ReLU()\n",
      "        (5): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (down): Identity()\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Call 5: สร้าง DataLoader + โมเดล DL (TCN) สำหรับ residual + wick\n",
    "# ============================\n",
    "\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X_seq, y):\n",
    "        self.X = torch.tensor(X_seq, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "\n",
    "train_ds = SeqDataset(Xs_train_s, yd_train)\n",
    "val_ds = SeqDataset(Xs_val_s, yd_val)\n",
    "test_ds = SeqDataset(Xs_test_s, yd_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k=3, dilation=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        pad = (k - 1) * dilation\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(c_in, c_out, k, padding=pad, dilation=dilation),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(c_out, c_out, k, padding=pad, dilation=dilation),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.down = nn.Conv1d(c_in, c_out, 1) if c_in != c_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        y = y[..., : x.size(-1)]  # causal crop\n",
    "        return y + self.down(x)\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, n_features, n_outputs=5, channels=(64, 64, 64)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        c_in = n_features\n",
    "        for i, c_out in enumerate(channels):\n",
    "            layers.append(TCNBlock(c_in, c_out, dilation=2**i))\n",
    "            c_in = c_out\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.head = nn.Linear(channels[-1], n_outputs)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, F] -> [B, F, T]\n",
    "        x = x.transpose(1, 2)\n",
    "        z = self.tcn(x)\n",
    "        z_last = z[..., -1]\n",
    "        raw = self.head(z_last)  # [B,5]\n",
    "\n",
    "        # non-inplace: สร้างคอลัมน์ใหม่\n",
    "        res_part = raw[:, 0:3]\n",
    "        uw = self.softplus(raw[:, 3:4])\n",
    "        lw = self.softplus(raw[:, 4:5])\n",
    "\n",
    "        out = torch.cat([res_part, uw, lw], dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = TCN(n_features=F, n_outputs=5).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9e65bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | train 0.01780 | val 0.00178\n",
      "Epoch   5 | train 0.00093 | val 0.00020\n",
      "Epoch  10 | train 0.00018 | val 0.00005\n",
      "Epoch  15 | train 0.00005 | val 0.00002\n",
      "Epoch  20 | train 0.00002 | val 0.00001\n",
      "Epoch  25 | train 0.00002 | val 0.00001\n",
      "Epoch  30 | train 0.00001 | val 0.00001\n",
      "Epoch  35 | train 0.00001 | val 0.00001\n",
      "Epoch  40 | train 0.00001 | val 0.00001\n",
      "Epoch  45 | train 0.00001 | val 0.00001\n",
      "Epoch  50 | train 0.00001 | val 0.00001\n",
      "Epoch  55 | train 0.00001 | val 0.00001\n",
      "Epoch  60 | train 0.00001 | val 0.00001\n",
      "Epoch  65 | train 0.00001 | val 0.00001\n",
      "Epoch  70 | train 0.00001 | val 0.00001\n",
      "Early stopping.\n",
      "[DL test] MAE: {'res_gap': np.float32(0.0011957579), 'res_range': np.float32(0.0028181921), 'res_body': np.float32(0.0039695823), 'upper_wick_next': np.float32(0.0015001134), 'lower_wick_next': np.float32(0.0013984661)}\n",
      "[DL test] RMSE: {'res_gap': np.float32(0.0016117169), 'res_range': np.float32(0.0037278363), 'res_body': np.float32(0.0053097545), 'upper_wick_next': np.float32(0.0021820408), 'lower_wick_next': np.float32(0.0018946237)}\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Call 6: Train DL (early stopping) + evaluate residual space\n",
    "# ============================\n",
    "\n",
    "best_val = 1e9\n",
    "best_state = None\n",
    "patience, wait = 20, 0\n",
    "max_epochs = 200\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    tr_losses = []\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    # ---- val ----\n",
    "    model.eval()\n",
    "    va_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            va_losses.append(loss.item())\n",
    "\n",
    "    tr_loss = float(np.mean(tr_losses))\n",
    "    va_loss = float(np.mean(va_losses))\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        wait = 0\n",
    "        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:3d} | train {tr_loss:.5f} | val {va_loss:.5f}\")\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "\n",
    "# ---- eval residual+wick predictions ----\n",
    "def predict_dl(loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            pred = model(xb).cpu().numpy()\n",
    "            preds.append(pred)\n",
    "            trues.append(yb.numpy())\n",
    "    return np.concatenate(preds), np.concatenate(trues)\n",
    "\n",
    "\n",
    "dl_pred_test, dl_true_test = predict_dl(test_loader)\n",
    "\n",
    "dl_names = [\"res_gap\", \"res_range\", \"res_body\", \"upper_wick_next\", \"lower_wick_next\"]\n",
    "mae_dl = mean_absolute_error(dl_true_test, dl_pred_test, multioutput=\"raw_values\")\n",
    "rmse_dl = np.sqrt(\n",
    "    mean_squared_error(dl_true_test, dl_pred_test, multioutput=\"raw_values\")\n",
    ")\n",
    "print(\"[DL test] MAE:\", dict(zip(dl_names, mae_dl)))\n",
    "print(\"[DL test] RMSE:\", dict(zip(dl_names, rmse_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "77a92672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FINAL OHLC test - ALIGNED]\n",
      "Open  MAE/RMSE: 0.001195757478553965 0.0016117184871266151\n",
      "High  MAE/RMSE: 0.00316229766539879 0.004590333359410505\n",
      "Low   MAE/RMSE: 0.0028585699776050725 0.004015433682250394\n",
      "Close MAE/RMSE: 0.003884972620721231 0.005273697946394204\n",
      "\n",
      "✔ Saved models to: /Users/thanaporn/Desktop/EURO_H1_AI/prepared_datasets/boosting_dl_residual/trained_models\n",
      " - LightGBM x3\n",
      " - TCN residual\n",
      " - DL scaler\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Call 7 (UPDATED): Align test preds -> Reconstruct OHLC -> Final metrics -> Save models\n",
    "# ============================\n",
    "\n",
    "feature_cols = meta[\"feature_cols\"]\n",
    "close_pos = feature_cols.index(\"Close\")\n",
    "seq_len = meta[\"seq_len\"]\n",
    "offset = seq_len - 1\n",
    "\n",
    "# --- 1) รวม tabular ทั้งหมด + boosting pred ทั้งชุด (เหมือน Call 4) ---\n",
    "Xb_all = np.concatenate([Xb_train, Xb_val, Xb_test], axis=0)\n",
    "yb_pred_all = predict_boost(Xb_all)  # [N_tab, 3]\n",
    "\n",
    "# --- 2) รวม DL true targets ทั้งหมดเพื่อหา N_dl และ slice pred ---\n",
    "yd_true_all = np.concatenate([yd_train_true, yd_val_true, yd_test_true], axis=0)\n",
    "N_dl = len(yd_true_all)\n",
    "\n",
    "yb_pred_dl_all = yb_pred_all[offset : offset + N_dl]\n",
    "assert len(yb_pred_dl_all) == N_dl\n",
    "\n",
    "# --- 3) แยก yb_pred ที่ align แล้วกลับเป็น train/val/test ตาม DL lengths ---\n",
    "n_tr = len(yd_train_true)\n",
    "n_va = len(yd_val_true)\n",
    "n_te = len(yd_test_true)\n",
    "\n",
    "yb_pred_train_aligned = yb_pred_dl_all[:n_tr]\n",
    "yb_pred_val_aligned = yb_pred_dl_all[n_tr : n_tr + n_va]\n",
    "yb_pred_test_aligned = yb_pred_dl_all[n_tr + n_va :]\n",
    "\n",
    "assert yb_pred_test_aligned.shape[0] == n_te == dl_pred_test.shape[0]\n",
    "\n",
    "# --- 4) close_t สำหรับ DL test ต้อง align ช่วงเดียวกับ DL test ด้วย ---\n",
    "# tabular rows ที่ตรงกับ DL ทั้งชุดคือ offset : offset+N_dl\n",
    "close_t_all = Xb_all[:, close_pos]\n",
    "close_t_dl_all = close_t_all[offset : offset + N_dl]\n",
    "\n",
    "close_t_test_aligned = close_t_dl_all[n_tr + n_va :]  # ยาวเท่า DL test\n",
    "assert close_t_test_aligned.shape[0] == n_te\n",
    "\n",
    "# --- 5) รวม struct preds บน test (boost aligned + dl residual) ---\n",
    "struct_pred_test = yb_pred_test_aligned + dl_pred_test[:, 0:3]  # [N_te,3]\n",
    "gap_pred, range_pred, body_pred = (\n",
    "    struct_pred_test[:, 0],\n",
    "    struct_pred_test[:, 1],\n",
    "    struct_pred_test[:, 2],\n",
    ")\n",
    "uw_pred, lw_pred = dl_pred_test[:, 3], dl_pred_test[:, 4]\n",
    "\n",
    "# --- 6) reconstruct OHLC preds ---\n",
    "open_next_pred = close_t_test_aligned + gap_pred\n",
    "close_next_pred = open_next_pred + body_pred\n",
    "high_next_pred = np.maximum(open_next_pred, close_next_pred) + uw_pred\n",
    "low_next_pred = np.minimum(open_next_pred, close_next_pred) - lw_pred\n",
    "\n",
    "# --- 7) reconstruct OHLC true (ใช้ DL true + close_t ที่ align แล้ว) ---\n",
    "gap_true, range_true, body_true = (\n",
    "    yd_test_true[:, 0],\n",
    "    yd_test_true[:, 1],\n",
    "    yd_test_true[:, 2],\n",
    ")\n",
    "uw_true, lw_true = yd_test_true[:, 3], yd_test_true[:, 4]\n",
    "\n",
    "open_next_true = close_t_test_aligned + gap_true\n",
    "close_next_true = open_next_true + body_true\n",
    "high_next_true = np.maximum(open_next_true, close_next_true) + uw_true\n",
    "low_next_true = np.minimum(open_next_true, close_next_true) - lw_true\n",
    "\n",
    "# --- 8) metric ---\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "def mae(a, b):\n",
    "    return float(mean_absolute_error(a, b))\n",
    "\n",
    "\n",
    "def rmse(a, b):\n",
    "    return float(np.sqrt(mean_squared_error(a, b)))\n",
    "\n",
    "\n",
    "print(\"\\n[FINAL OHLC test - ALIGNED]\")\n",
    "print(\n",
    "    \"Open  MAE/RMSE:\",\n",
    "    mae(open_next_true, open_next_pred),\n",
    "    rmse(open_next_true, open_next_pred),\n",
    ")\n",
    "print(\n",
    "    \"High  MAE/RMSE:\",\n",
    "    mae(high_next_true, high_next_pred),\n",
    "    rmse(high_next_true, high_next_pred),\n",
    ")\n",
    "print(\n",
    "    \"Low   MAE/RMSE:\",\n",
    "    mae(low_next_true, low_next_pred),\n",
    "    rmse(low_next_true, low_next_pred),\n",
    ")\n",
    "print(\n",
    "    \"Close MAE/RMSE:\",\n",
    "    mae(close_next_true, close_next_pred),\n",
    "    rmse(close_next_true, close_next_pred),\n",
    ")\n",
    "\n",
    "# --- 9) save models ---\n",
    "MODEL_DIR = DATA_DIR / \"trained_models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import joblib\n",
    "\n",
    "for i, m in enumerate(boost_models):\n",
    "    joblib.dump(m, MODEL_DIR / f\"lgb_{target_names[i]}.pkl\")\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_DIR / \"tcn_residual.pth\")\n",
    "joblib.dump(scaler, MODEL_DIR / \"dl_scaler.pkl\")\n",
    "\n",
    "print(\"\\n✔ Saved models to:\", MODEL_DIR)\n",
    "print(\" - LightGBM x3\")\n",
    "print(\" - TCN residual\")\n",
    "print(\" - DL scaler\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forex-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
